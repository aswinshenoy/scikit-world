{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Scikit Learn\n",
    "### Diving into Machine Learning, NLP, Text Classification Scikit Learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am going to do text-classification using Scikit Learn, on the popular `20NewsGroups` dataset.\n",
    "\n",
    "#### How Supervised *Machine* Learning works?\n",
    "1. Training the machine with data which is well-labeled i.e. already tagged with the correct answer.\n",
    "2. Providing the machine with a new set of examples (data), so that supervised learning algorithm analyses the training data (set of training examples) and produces a correct outcome from labeled data. Basically, how much the data is matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Dataset - 20NewsGroups\n",
    "\n",
    "I am going to use the popular `20NewsGroups` dataset available in scikit learn library, which is widely used in different beginner tutorials including the one's I have referred.\n",
    "\n",
    "The popular 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "\n",
    "* The tutorial I refered - [towardsdatascience.com](https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a).\n",
    "* You can read more about this dataset here: - http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_data.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Names (Categories of Our Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our data is organized into 20 different newsgroups, each corresponding to a different topic. \n",
    "* Some of the newsgroups are very closely related to each other, while others are highly unrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\n",
      "Subject: Need info on 88-89 Bonneville\n",
      "Organization: University at Buffalo\n",
      "Lines: 10\n",
      "News-Software: VAX/VMS VNEWS 1.41\n",
      "Nntp-Posting-Host: ubvmsd.cc.buffalo.edu\n",
      "\n",
      "\n",
      " I am a little confused on all of the models of the 88-89 bonnevilles.\n",
      "I have heard of the LE SE LSE SSE SSEI. Could someone tell me the\n",
      "differences are far as features or performance. I am also curious to\n",
      "know what the book value is for prefereably the 89 model. And how much\n",
      "less than book value can you usually get them for. In other words how\n",
      "much are they in demand this time of year. I have heard that the mid-spring\n",
      "early summer is the best time to buy.\n",
      "\n",
      "\t\t\tNeil Gandler\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(training_data.data[0])\n",
    "print(testing_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the first observation of the dataset, for both training and testing. As we can see, they are very much alike in format. Infact, thus, our machine should be able to predict they are highly similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **What We Have?**  - Text files/datasets are collections of words\n",
    "* **What We Need?**  - To run ML-algorithms, we need to convert text into numerical-feature vectors.\n",
    "\n",
    "### Converting to Term-Document Matrix using Bag of Words Model\n",
    "\n",
    "#### What is the `Bag of Words` Model?\n",
    "Bag of Words model is simplifying representation of text, in which some text is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "You can read more about it in [wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "\n",
    "#### What we are going to do?\n",
    "1. Segment each text file into words (for English splitting by space)\n",
    "2. Count occurence of each word in each document\n",
    "3. Assign each word an integer id\n",
    "4. Each unique word in our dictionary will correspond to a feature (descriptive feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scikit learn libary provides a method which can convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tds_count = count_vect.fit_transform(training_data.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit_tranform function learns the vocabulary dictionary, and then returns term-document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tds_count` variable is an matrix in the format [n_samples, n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Transformation\n",
    "\n",
    "#### TF (Term Frequency) \n",
    "* The issue when we just count the number of words in each document, is that it will give more weightage to longer documents than shorter documents. \n",
    "* For avoiding this, we can use frequency (TF - Term Frequencies) adjusted for document length:\n",
    "\n",
    "$$\n",
    "        \\frac{\\text{Count of Word in document}}{\\text{Total words in document}}\n",
    "$$\n",
    "\n",
    "\n",
    "#### TF - IDF (Term Frequency - Inverse Document Frequency)\n",
    "* We can also reduce the weightages of more common words (such as `the`, `is`, `an` etc.), which occurs more frequently in a collection of documents compared to others.\n",
    "* This is done through TF-IDF i.e Term Frequency times inverse document frequency.\n",
    "\n",
    "You can read more about it in [wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tds_tfidf = tfidf_transformer.fit_transform(tds_count)\n",
    "tds_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction using Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be now using `Pipeline` function of the scikit learn library which allows us to run multiple tasks such as extraction of text features, TF-IDF transformation and classification all one after the other, much easily.\n",
    "* The numpy libary is gonna be useful for us to compare our prediction with the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = Pipeline([\n",
    "                        ('vect', CountVectorizer()), \n",
    "                        ('tfidf', TfidfTransformer()), \n",
    "                        ('nb_classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "nb_classifier = nb_classifier.fit(training_data.data, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = nb_classifier.predict(testing_data.data)\n",
    "np.mean(prediction == testing_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDGC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sdgc_classifier = Pipeline([\n",
    "                        ('vect', CountVectorizer()), \n",
    "                        ('tfidf', TfidfTransformer()), \n",
    "                        ('sdgc_clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42))\n",
    "])\n",
    "\n",
    "sdgc_classifier = sdgc_classifier.fit(training_data.data, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8238183749336165"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sdgc_classifier.predict(testing_data.data)\n",
    "np.mean(prediction == testing_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improving Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "* Stop words are usually the most common words in a language\n",
    "* We can filter those out if they are not relevant for the underlying problem.\n",
    "\n",
    "Read more at [wikipedia](https://en.wikipedia.org/wiki/Stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sw_classifier = Pipeline([\n",
    "                        ('vect', CountVectorizer(stop_words='english')), \n",
    "                        ('tfidf', TfidfTransformer()), \n",
    "                        ('nb_classifier', MultinomialNB())\n",
    "])\n",
    "nb_sw_classifier = nb_sw_classifier.fit(training_data.data, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8169144981412639"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = nb_sw_classifier.predict(testing_data.data)\n",
    "np.mean(prediction == testing_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy has improved to 81.69% from 77.38% earlier for the NB algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdgc_sw_classifier = Pipeline([\n",
    "                        ('vect', CountVectorizer(stop_words='english')), \n",
    "                        ('tfidf', TfidfTransformer()), \n",
    "                        ('sdgc_clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42))\n",
    "])\n",
    "sdgc_sw_classifier = sdgc_sw_classifier.fit(training_data.data, training_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224907063197026"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sdgc_sw_classifier.predict(testing_data.data)\n",
    "np.mean(prediction == testing_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the performance of SDGC alogrithm almost remained the same, or rather slightly decreased to 82.38% from 82.24% earlier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
